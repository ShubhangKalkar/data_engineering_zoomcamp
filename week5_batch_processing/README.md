# Week 5 â€” Data Platforms with Bruin

Overview

In Week 5, I implemented a declarative data pipeline using Bruin to process NYC Taxi data end-to-end â€” from ingestion to transformed analytical tables â€” using a lightweight local analytics engine (DuckDB).

This week focused on:
```
1. Data platform concepts (assets, lineage, contracts)
2. Declarative pipeline design
3. Incremental/time-based materialization
4. Built-in data quality checks
5. Reproducible analytics pipelines
```
```
Architecture
Raw Data (NYC Taxi Files)
        â†“
Bruin Assets (Python + SQL)
        â†“
DuckDB Processing Layer
        â†“
Cleaned / Deduplicated Staging Tables
        â†“
Analytics Tables
        â†“
Data Quality Checks + Lineage Graph
```

Bruin acts as:

1. Orchestrator
2. Transformation engine
3. Dependency manager
4. Data quality validator

```
ğŸ“‚ Project Structure
my-pipeline/
â”œâ”€â”€ .bruin.yml                # Connection configuration (DuckDB)
â””â”€â”€ pipeline/
    â”œâ”€â”€ pipeline.yml          # Pipeline definition + variables
    â””â”€â”€ assets/
        â”œâ”€â”€ ingestion/        # Data ingestion logic
        â”œâ”€â”€ staging/          # Cleaning & deduplication
        â””â”€â”€ marts/            # Analytical models
```

Required Components

Bruin projects must contain:
1. .bruin.yml
2. pipeline/pipeline.yml
3. pipeline/assets/

âš™ï¸ Setup Instructions
1ï¸âƒ£ Install Bruin CLI
curl -LsSf https://getbruin.com/install/cli | sh

Verify installation:

bruin --version
2ï¸âƒ£ Initialize Project
bruin init zoomcamp my-pipeline
cd my-pipeline
3ï¸âƒ£ Configure DuckDB Connection

Edit .bruin.yml:

connections:
  duckdb:
    type: duckdb
    path: zoomcamp.duckdb

This creates a local analytical warehouse (no cloud required).

4ï¸âƒ£ Run the Pipeline
bruin run

First run should use:

bruin run --full-refresh

This ensures tables are created from scratch.

ğŸ§  Materialization Strategy

For monthly NYC taxi data, we used:

time_interval

This allows:

Incremental processing

Partition-aware transformations

Efficient reprocessing by pickup_datetime

ğŸ” Running Specific Assets with Dependencies

If an upstream file changes:

bruin run --select ingestion.trips+

The + runs all downstream dependencies automatically.

ğŸ“Š Data Quality Checks

Example check added to ensure required fields:

columns:
  pickup_datetime:
    not_null: true

Bruin automatically validates data contracts during execution.

ğŸ”§ Pipeline Variables

Defined inside pipeline.yml:

variables:
  taxi_types:
    default: ["yellow", "green"]

Override at runtime:

bruin run --var 'taxi_types=["yellow"]'
ğŸ“ˆ Visualizing Lineage

Bruin provides built-in lineage visualization:

bruin lineage

This generates a dependency graph of the pipeline.